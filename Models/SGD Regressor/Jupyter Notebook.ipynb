{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SGDRegressor \n",
        "\n",
        "<img src = \"data:image/png;base64,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\">\n",
        "\n",
        "SGD Regressor is a type of regression algorithm that uses Stochastic Gradient Descent (SGD) optimization to find the coefficients that minimize the mean squared error loss function.\n",
        "\n",
        "The basic idea behind SGD is to update the model's parameters (coefficients) in small batches, rather than using the entire dataset at once. This makes it faster and more efficient than batch gradient descent, especially for large datasets.\n",
        "\n",
        "In SGD Regressor, the algorithm starts with random coefficients and then updates them iteratively, taking small steps in the direction of the negative gradient of the loss function. This process continues until the algorithm reaches a minimum point or a stopping criterion is met.\n",
        "\n",
        "One important hyperparameter in SGD Regressor is the learning rate, which determines the size of the steps taken during each iteration. If the learning rate is too high, the algorithm may overshoot the minimum point and fail to converge, while if it is too low, the algorithm may take too many iterations to converge.\n",
        "\n",
        "SGD Regressor is commonly used in machine learning applications such as linear regression, support vector regression, and logistic regression. It is also popular for online learning, where the model is updated continuously as new data becomes available."
      ],
      "metadata": {
        "id": "lRWQ0jC9sLU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np "
      ],
      "metadata": {
        "id": "hpO6CtSgt8PP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GYFATNVnt7t4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know gradient descent wants weights and biases for a staring point, lets assume we have two paramters, these two parameters will have \n",
        " weights and \n",
        " biases\n",
        "\n",
        "What to remember while weights intialization\n",
        "\n",
        "* From experiments it is found, when weights are intialsed as \n",
        ", they do not change at all\n",
        "* From experiments it is found that weight intialization of random numbers but wiht huge difference is a bad habit\n",
        "* From experimaent it is found that weight intialization of random numbers but with very little difference is a bad habit"
      ],
      "metadata": {
        "id": "u9f1dndHtNno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.random.rand(2) * 0.1\n",
        "baises = np.random.rand(1) * 0.1"
      ],
      "metadata": {
        "id": "-XPCRo6Xt3JB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgQHl9_ruJuP",
        "outputId": "140e12a8-95fe-4d47-99e0-40470afdcfa3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01914039, 0.04479674])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baises"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFn0ITw5uLFo",
        "outputId": "96df510d-7b2d-4766-8fea-f44ce2750ad3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.04396207])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets store both of these values into another array `params`"
      ],
      "metadata": {
        "id": "yiLtmc-juO7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = np.array([weights , baises])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRJkkbXPuSvQ",
        "outputId": "8393f5a9-b789-43e9-e273-087e4f6d2f58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-74f37a80a9f8>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  params = np.array([weights , baises])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx5atp_huWBn",
        "outputId": "acf3ecca-53e8-4519-eae2-db232fcc71b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([array([0.01914039, 0.04479674]), array([0.04396207])], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we know gradient is nothing but the derivative of the slopes, the slopes are in the form $x^2$, so there derivative will be $2x$ as"
      ],
      "metadata": {
        "id": "OEpLEYMWucpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need to update the gradients, for this, we need th elearning rate and a new term, loss. Lets assume loss here is 2"
      ],
      "metadata": {
        "id": "LTYBoPnSvfM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "loss = 2"
      ],
      "metadata": {
        "id": "FhXKEVYJv1L-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights -= lr*loss\n",
        "baises -= lr*loss"
      ],
      "metadata": {
        "id": "a8UXNEaoveun"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we have created our sgd, now we just need to put all of this into a function "
      ],
      "metadata": {
        "id": "dQlRBWzAv4GW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq1bzvDyr-tT"
      },
      "outputs": [],
      "source": [
        "def SGD(X_train , Y_train , epochs = 100 , lr = 0.01):\n",
        "    \n",
        "    def initialize(number_of_features):\n",
        "    \n",
        "        weights = np.random.randn(number_of_features) * 0.1\n",
        "        biases = np.random.randn(1) * 0.1\n",
        "\n",
        "        params = np.array([weights , biases])\n",
        "\n",
        "    def predict(weights , biases , X_test):\n",
        "        \n",
        "        predicted_values = (weights * X_test) + biases\n",
        "\n",
        "        return predicted_values\n",
        "\n",
        "    def backward(Y_train , predicted):\n",
        "    \n",
        "        loss = (Y_train - predicted) ** 2\n",
        "    \n",
        "        return loss\n",
        "    \n",
        "    def forward(X_train , Y_train , epochs , lr):\n",
        "    \n",
        "        initialize(len(X_train.columns))\n",
        "    \n",
        "        for _ in range(epochs):\n",
        "    \n",
        "            predict(weights , biases , X_train)\n",
        "            backward(Y_train , predicted_values)\n",
        "    \n",
        "            weights = weights - (lr * loss)\n",
        "            biases = biases - (lr * loss)\n",
        "\n",
        "    forward(X_train , Y_train , epochs , lr)\n",
        "\n",
        "    new_params = np.array([weights , biases])\n",
        "\n",
        "    return new_params"
      ]
    }
  ]
}