<a href="https://www.kaggle.com/code/ayushs9020/gaussian-naive-bayes-from-scratch"><img src = "https://cdn.iconscout.com/icon/free/png-256/free-kaggle-3521526-2945029.png" width = 50>

# Gaussian Naive Bayes

Gaussian Naive Bayes is a popular machine learning algorithm used for classification tasks. It is based on the principles of Bayes' theorem and assumes that the features in the dataset are independent of each other. In Gaussian Naive Bayes, each feature is assumed to follow a Gaussian or normal distribution. This means that the algorithm assumes that the continuous values of each feature in the dataset are generated from a Gaussian distribution. If the features do not follow a Gaussian distribution, the algorithm may not perform well. The algorithm calculates the posterior probability of each class given the features using Bayes' theorem. The posterior probability is the probability of a class given the observed features. The class with the highest posterior probability is assigned as the predicted class for a given input.

To estimate the parameters of the Gaussian distribution for each feature in each class, the algorithm calculates the mean and standard deviation of the feature values in the training dataset for each class. These parameters are then used to model the probability density function (PDF) of each feature in each class. During the prediction phase, the algorithm calculates the likelihood of the observed feature values given each class using the PDF of the Gaussian distribution. It then multiplies the prior probability of each class with the likelihood to obtain the unnormalized posterior probabilities. The final step involves normalizing the posterior probabilities to ensure that they sum up to one, representing valid probabilities. Gaussian Naive Bayes is a simple and efficient algorithm that works well when the assumption of feature independence holds true. It is particularly useful when dealing with high-dimensional datasets, and it performs well even with limited training data. However, it may not be suitable for datasets with highly correlated features or when the assumption of a Gaussian distribution is violated.
