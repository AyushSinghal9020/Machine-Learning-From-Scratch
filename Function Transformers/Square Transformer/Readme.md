# Square Transformer
Square Transformer is a type of transformer architecture commonly used in natural language processing (NLP) tasks such as language modeling, machine translation, and sentiment analysis. It is based on the original trance he architecture described in the article "Attention Is All You Need" by Vaswani et al. Suggested. (2017).

In Square Transformer, the input sequence is first embedded in a high-dimensional vector space. The embedding is then processed by a stack of transformer layers, each consisting of a multi-headed self-recognition mechanism and a feedforward neural network. The self-recognition mechanism allows the model to focus on different parts of the input sequence, while the feedforward network applies nonlinear transformations to the representations learned by the self-recognition layer.

One of Square Transformer's key features is that it uses positional encoding to provide the model with information about the position of each token in the input sequence. This is necessary because the transformer doesn't use iteration or convolution to process the sequence, so it can't directly capture the order of the tokens.

Square Transformer delivers state-of-the-art performance on a variety of NLP tasks, including language modeling, machine translation, and sentiment analysis. It has also been applied to other domains such as image processing and speech recognition. 
