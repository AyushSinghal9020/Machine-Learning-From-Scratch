The F-1 score is a metric commonly used to evaluate the performance of a binary classification model. It is the harmonic mean of precision and recall, two important metrics in evaluating classification models.

Precision measures the proportion of true positive predictions over all positive predictions, while recall measures the proportion of true positive predictions over all actual positive instances in the dataset.

The F-1 score combines precision and recall into a single metric that represents the balance between the two metrics. It ranges from 0 to 1, with a score of 1 indicating perfect precision and recall, and a score of 0 indicating poor performance.

The F-1 score is particularly useful when the distribution of positive and negative instances in the dataset is imbalanced, as it takes into account both false positives and false negatives.

Overall, the F-1 score is a useful metric for evaluating binary classification models, especially when the balance of positive and negative instances in the dataset is uneven. $$f1_-Score = \frac {2}{\frac {1}[recall} + \frac {1}{precision}}$$
